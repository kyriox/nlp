{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk, spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulario\n",
    "\n",
    "El primer paso dentro del procesmaiento de lenguage natural implica la construcción de un vocabulario. El objetivo es convertir secuencias de texto en unidades *minimas* de escritura con *significado* (tokens). Para este curso usaremos palabras, $n$-gramas y $q$-gramas. Sin embargo, las técnicas de preprocesamiento pueden ser extendidas facilmente a equaciones, emoticones o cualquier otra unidad de escritura.   \n",
    "\n",
    "Como lidiamos con lenguaje escrito, la obtención de los tokens requiere manipulación de cadenas de caracteres. Se requiere identificar puntuación, signo diacríticos (dieresis, acententos), en el caso del Inglés es posible que se desee dividir las contracciones (You're -> you are). Una vez construido el vocabulario mediante la **tokenización** de todos los documentos, puede  realizarse una reducción del mismo mediante un proceso de **lematización** o **steamming**. Ya con el vocabulario es posible construir una representación vectorial. \n",
    "\n",
    "Note que la tokenización podría ser a nível de sufijos/prefijos, silabas o incluso letras, pero por el momento solo lidiaremos con palabras. También es posible construir unidades formadas por 2,3 o $n$ palabras, a esto tokens se les conoce como $n$-gramas y nos permitene incluir conceptos que de más de una unidad por ejemplo en inglés *ice cream*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizador\n",
    "\n",
    "Es proceso de tokenizado es un proceso de segmentación de *documentos*. Donde la segmentación es dividir el texto (información no estructurada) en unidades más pequeñas que pueden ser contabilizadas de forma discreta. El resultado de la contabilización de las ocurrencias de cada término puede ser utilizada directamente como un representación vectorial del documento. Con lo cual se transforma una entrada de información no estructurada en información estructurada que puedes ser utilizada por algoritmos de aprendizaje automático. La aplicación más común de este tipo de vectores (**bag of words**) para recuperación de documentos o búsqueda. \n",
    "\n",
    "El tokenizador más simple consiste en utilizar el espacio en blanco como delimitador para definir los tokens en terminos de palabras. En python se puede hacer como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pepe',\n",
       " 'pecas',\n",
       " 'pica',\n",
       " 'papas',\n",
       " 'con',\n",
       " 'un',\n",
       " 'pico,',\n",
       " 'con',\n",
       " 'un',\n",
       " 'pico',\n",
       " 'pepe',\n",
       " 'pecas',\n",
       " 'pica',\n",
       " 'papas.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"Pepe pecas pica papas con un pico, con un pico pepe pecas pica papas.\"\n",
    "doc.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el  método *split* pareciera se tiene un tokenizador medianamente bueno, sin embargo hay al menos dos situaciones no deseadas la primera los tokes *pico,*  y *papas.* esos tokens incluyen signos de puntuación, además los tokens como *Pepe* y *pepe* serán considerados como elementos distintos. Un tokenizador mas soifisticado debería separar los tokens de la puntuación, por ahora dejaremos este cuestión pero la retomaremos más adelante. Una forma básica de obtener una representación númerica de una secuencia de texto es mediante una representación binaria de cada token que existe en el vocabulario, está representación es conocida como **one-hot vectors**. Cada sentencia es representada como una lista de one-hot vectors por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del vocabulario: 7\n"
     ]
    }
   ],
   "source": [
    "## obtenemos el vocabulario a partir\n",
    "doc1=\"Pepe pecas pica papas con un pico\"\n",
    "vocabulario=str.split(doc1) #Utilizamos el tokenizador\n",
    "## se ordena\n",
    "vocabulario.sort()\n",
    "n=len(vocabulario)\n",
    "print(f\"El tamaño del vocabulario: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pepe', 'con', 'papas', 'pecas', 'pica', 'pico', 'un']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada one-hot vector binaria será del tamaño del vocabulario y tendrá solo un 1 en la posición que corresponde a la palabra que representa.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "## Creamos la tabla de one-hot vectors \n",
    "one_hot_vectors=np.zeros((n,n)) \n",
    "## generamos la representación vectorial para nuesta frase de ejemplo\n",
    "for i,w in enumerate(doc1.split()):\n",
    "    one_hot_vectors[i,vocabulario.index(w)]=1\n",
    "one_hot_vectors=one_hot_vectors.astype(int)\n",
    "print(one_hot_vectors) # ya tenemos un vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pepe</th>\n",
       "      <th>con</th>\n",
       "      <th>papas</th>\n",
       "      <th>pecas</th>\n",
       "      <th>pica</th>\n",
       "      <th>pico</th>\n",
       "      <th>un</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pepe</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pecas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pica</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>papas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>un</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pico</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pepe con papas pecas pica pico un\n",
       "Pepe     1                             \n",
       "pecas                    1             \n",
       "pica                          1        \n",
       "papas              1                   \n",
       "con          1                         \n",
       "un                                    1\n",
       "pico                               1   "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Podemos ver la misma información en un dataFrame para hacerlo un poco mas legible\n",
    "df = pd.DataFrame(one_hot_vectors, columns=vocabulario, index=doc1.split())\n",
    "df[df == 0] = '' #remplazamos los 0 con la cadena vacia\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para nuestro ejemplo tenemos una matriz de 7x7 ya que el vocabulario está solo constituido por una única sentencia. Recuerde que un 1 indica que el token si es parte del documento y un 0 que ese temino no se encuentra en el mismo. Este tipo de estructura es eficiente para determinar si un palabra es o no parte del un documento (solo debe verse si la fila está activa en la columna correspondiente). Además siempre es posible reconstruir el documento original, con lo cual no se pierde información. Este tipo de representación es frecuentemente utilizado en redes neuronales y modelado de lenguajes. \n",
    "\n",
    "La importancia de este tipo de representación es que se ha transformado una sentencia escrita en lenguaje natural, a un espacio donde es posible que una *máquina* realice operaciones matemáticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las desventajas de tener una representación matricial de los one-hot vectors es que debido a que son altamenete disperso, su almacenamiento puede ser ineficianete si se hace de forma matricial; mientras que si lo hacemos mediante listas o alguna otra estructura dispersa se tiene un incremento en la complejida de las operaciones. \n",
    "\n",
    "En el caso del idioma castellano se considera que existen al rededor de unas 100,000 palabras. Teniendo lo anterior en cuenta, un one-hot vector para una palabra dada tendría que ser un vector de dimensión 10000 con único elemento diferente de 0. \n",
    "<div class=\"alert alert-success\">\n",
    "    <ul>\n",
    "<li>¿Que costo tendría almacenar nuestra frase de ejemplo sin utilizar una representación dispersa que considere todas las palabras del idioma castellano? </li>\n",
    "<li>¿Un documento con 100 tokens?</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000000000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*20000*4*1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considere que se utilizan 4 bytes por caracter o entero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "Usar un representación one-hot requiere grandes cantidades de memoria, otro posible enfoque es sumarizar la información en la tabla en un solo vector. Lo anterior reduciria la memoria requerida pero se perdería la información del orden en que aparecen en el documento (de ahi el nombre de bag of words). Aún con la desventaja anterior este vector conservaria los conceptos que aparecen en el documento, sería semejante a un índice de terminos en un libro.\n",
    "\n",
    "La sumarización puede realizarse mediante un vector binario que solo indique si la palabra aparece o no en el documento, o bien contando el número de veces que aparece cada término (vector de frecuencias)\n",
    "\n",
    "Considere el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pepe</th>\n",
       "      <th>con</th>\n",
       "      <th>papas</th>\n",
       "      <th>pecas</th>\n",
       "      <th>pica</th>\n",
       "      <th>pico</th>\n",
       "      <th>un</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pecas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pica</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>papas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>un</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pico</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>un</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pico</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pecas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pica</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>papas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pepe con papas pecas pica pico un\n",
       "pecas                    1             \n",
       "pica                          1        \n",
       "papas              1                   \n",
       "con          1                         \n",
       "un                                    1\n",
       "pico                               1   \n",
       "con          1                         \n",
       "un                                    1\n",
       "pico                               1   \n",
       "pecas                    1             \n",
       "pica                          1        \n",
       "papas              1                   "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2=\"pecas pica papas con un pico con un pico pecas pica papas\"\n",
    "## Creamos la tabla de one-hot vectors \n",
    "one_hot_vectors2=np.zeros((len(doc2.split()),n)) \n",
    "## generamos la representación vectorial para nuesta frase de ejemplo\n",
    "for i,w in enumerate(doc2.split()):\n",
    "    one_hot_vectors2[i,vocabulario.index(w)]=1\n",
    "df2 = pd.DataFrame(one_hot_vectors2, columns=vocabulario, index=doc2.split())\n",
    "df2[df2 == 0] = '' #remplazamos los 0 con la cadena vacia\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el one-hot encoding podemos generar el vector binario como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(one_hot_vectors2, axis=0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "también con el la tabla one-hot podemos generar el vector de frecuencias como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(one_hot_vectors2, axis=0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La memoria requerida  por representación previa aún depende aún depende del tamaño del vocabulario y sigue siendo prohibitiva para grandes cantidades de datos. Una forma más ecónomica en memoria es utilizar una lista asociativa(diccionario) como  sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pepe': 1, 'pecas': 1, 'pica': 1, 'papas': 1, 'con': 1, 'un': 1, 'pico': 1}\n",
      "{'pecas': 1, 'pica': 1, 'papas': 1, 'con': 1, 'un': 1, 'pico': 1}\n"
     ]
    }
   ],
   "source": [
    "#version binaria\n",
    "doc1_bow = {token: 1 for token in doc1.split()}\n",
    "print(doc1_bow)\n",
    "doc2_bow = {token: 1 for token in doc2.split()}\n",
    "print(doc2_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario=['pepe','con','papas','pecas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[[(0,1),(3,1)],[(5,1),(10,4)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pecas': 2, 'pica': 2, 'papas': 2, 'con': 2, 'un': 2, 'pico': 2}\n"
     ]
    }
   ],
   "source": [
    "#version frecuencia\n",
    "doc2_bowf={}\n",
    "for token in doc2.split():\n",
    "   doc2_bowf[token]=doc2_bowf.get(token,0)+1\n",
    "print(doc2_bowf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pepe</th>\n",
       "      <th>pecas</th>\n",
       "      <th>pica</th>\n",
       "      <th>papas</th>\n",
       "      <th>con</th>\n",
       "      <th>un</th>\n",
       "      <th>pico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pepe  pecas  pica  papas  con  un  pico\n",
       "doc1     1      1     1      1    1   1     1\n",
       "doc2     0      1     1      1    1   1     1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow=pd.DataFrame([doc1_bow,doc2_bow], index=['doc1','doc2']).fillna(0).astype(int)\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almacenar a la información de esta última forma es mucho más eficiente en memoria ya que cada documento ya que solo se consideran las palabras persentes en el documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora practiquemos con un ejemplo que conste de una colección de documentos (corpus), considere el siguiente conjunto de sentencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\"el rey de constantinopla esta constantinoplizado.\",\n",
    "        \"consta que constanza no lo pudo desconstantinoplizar.\"\n",
    "        \"el desconstantinoplizador que desconstantinoplizare al rey de constantinopla\",\n",
    "        \"buen desconstantinoplizador será\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto punto\n",
    "\n",
    "El producto punto de dos vectores o producto escalar se calcula multiplicando todos los elementos de un vector por todos los elementos del segundo vector y luego sumando cada uno de los resultados del producto. \n",
    "\n",
    "En python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([5, 4, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.dot(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(v1*v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x1 * x2 for x1, x2 in zip(v1, v2)]) # ineficiente pero ilustrativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "¿Cómo podemos medir que tan similares son dos documentos?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La representación binaria de bag of words es un espacio vectorial (VSM) obtenido a partir de documentos en lenguaje natural (oraciones). En este espacio es posible realizar productos escalares, así como otras operaciones vectoriales como: suma, resta, *and*, *or*, medias, etc. También nos permite medir similitud/distancia entre documentos (i.e. distancia euclidiana,el ángulo entre vectores, etc). Como sabemos los procesadores utilizan expresiones binarias que son utilizadas para realizar indizado y realizar búsquedas de forma eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejorando el tokenizador\n",
    "\n",
    "Es frecuente, que se desee utilizar como separadores de tokens utilizan caracteres diferentes  a en una oración. Además de que nuestro rokenizador mantiene los signos de puntuación en las palabras. Una posible solución sería dividir el texto no solo en espacios en blanco, sino también en puntuación (comas, puntos, comillas, signos de amiración, etc). Sin embargo, en  algunos casos, podría desearse tratar lo signos como tokens independientes o tal vez simplemente se quiera ignorarlos.\n",
    "\n",
    "Una de incluir diferentes patrones de división es meidante el uso de expresiones regulares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expresiones regulares\n",
    "\n",
    "Recordemos que las expresiones sirver para expresar lenguajes regulares, y en Python se pueden utilizar mediante la librería **re**. Rvisaremos brevemente alguno aspectos del su uso:\n",
    "\n",
    "- Los corchetes ([x]) se utilizan para indicar una tipo o un conjunto de caracteres. \n",
    "- El signo + después del corchete de cierre (]) indica que debe haber al menos una coincidencia de los caracteres dentro de los corchetes. \n",
    "- El signo * indica que cero o mas coincidencias de los caracteres dentro de la clase. \n",
    "- El símbolo  \\s dentro es una clase predefinida que incluye todos los espacios en blanco como  [espacio], [tabulador]. Los seis caracteres de espacio en blanco son espacio (' '), tabulación ('\\ t'), return ('\\ r'), nueva línea ('\\n') y  ('\\f').\n",
    "- Para indicar un rango de caracteres se utiliza el signo menos (-). Por ejemplo \\[1-9\\] indica la clase  [123456789], [a-zA-Z] hace match con los rangos de minusculas y mayusculas de los caracteres alfanuméricos. \n",
    "- Los parentesis son utilizado para agrupar expresiones regulares.\n",
    "- Para expresar que se desea hacer match con -, debe ponerse justo después del corchete abierto para la clase de carácter. En caso contrario el analizador lo tomara como un rango de caracteres. \n",
    "- Los caracters especiales se pueden escapar utilizando una barra invertida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Python la biblioteca *re* permite compilar las expresiones regulares, con lo que se obtiene un tokenizador más eficiente. \n",
    "\n",
    "Las expresiones regulares también nos permiten realizar normalizaciones de texto complejas, por ejemplo extrar hyper-vínculos, direcciones de correo, nombres de usuario etc. Veremos un ejemplo sencillo de normalización y retomaremos el tema más adelante en el curso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "## Una expresión regular que divide utilizando signos de puntuación y espacios en blanco\n",
    "patron_tokenizer=re.compile(r\"([-\\s.,;¿?¡!])+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilizaremos la siguiente frase tomada del poema Día trece de Ramón Lopez Velarde\n",
    "poema=\"\"\"¿En qué embriaguez bogaban tus pupilas para que así pudiesen narcotizarlo todo? \n",
    "          Tu tiniebla guiaba mis latidos, cual guiabala columna de fuego al israelita.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=patron_tokenizer.split(poema)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '¿', 'En', ' ', 'qué', ' ', 'embriaguez', ' ', 'bogaban', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:10]) #Los últimos 13 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver tenemos tenemos espacios en blanco, por lo que requerimos filtar los caracteres que no sean de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tu', 'tiniebla', 'guiaba', 'mis', 'latidos', 'cual', 'guiabala', 'columna', 'de', 'fuego', 'al', 'israelita']\n"
     ]
    }
   ],
   "source": [
    "no_deseados=['-',' ','\\t','\\n','.',';',',','¿','?','¡','!','']\n",
    "tokens_sin_puntuacion=[x for x in tokens if x not in no_deseados]\n",
    "print(tokens_sin_puntuacion[-12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hace match con todos las cadena que comienzan con @ y contienen al menos un caracter más\n",
    "patron_user=re.compile(r\"(@[a-zA-Z0-9\\.]+)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet=\"\"\"Hey @elon.musk  this would be so cool in synergy with what we do \n",
    "@ExoWandercraft, already letting the walking impaired walk \n",
    "autonomously http://tinyurl.com/3zaj9xqs. Hit us up! ;)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey<user>  this would be so cool in synergy with what we do \n",
      "<user>, already letting the walking impaired walk \n",
      "autonomously http://tinyurl.com/3zaj9xqs. Hit us up!\n"
     ]
    }
   ],
   "source": [
    "tweetu=patron_user.sub('<user>',tweet)\n",
    "print(tweetu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EJERCICIO</b>:\n",
    "Definir una expresión regular que remplace los hipervínculos con la cadena <br /> \n",
    "&lt;link &gt;\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tokenizador puede ser tan complejo como se desee y puede querer adaptarse a una tarea especifíca. Por ejemplo en un tweet podríamos tratar de forma especial los caracteres XD. \n",
    "Existen muchas librerías en python que implementan tokenizadores especializados (dominio, idioma, etc). Las dos que mostraremos en este curso son **spaCy** y **NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " '@elon',\n",
       " '.',\n",
       " 'musk',\n",
       " 'this',\n",
       " 'would',\n",
       " 'be',\n",
       " 'so',\n",
       " 'cool',\n",
       " 'in',\n",
       " 'synergy',\n",
       " 'with',\n",
       " 'what',\n",
       " 'we',\n",
       " 'do',\n",
       " '@ExoWandercraft',\n",
       " ',',\n",
       " 'already',\n",
       " 'letting',\n",
       " 'the',\n",
       " 'walking',\n",
       " 'impaired',\n",
       " 'walk',\n",
       " 'autonomously',\n",
       " 'http://tinyurl.com/3zaj9xqs',\n",
       " '.',\n",
       " 'Hit',\n",
       " 'us',\n",
       " 'up',\n",
       " '!',\n",
       " ';)']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLTK tokenizador de tweers \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk_tkzr=TweetTokenizer()\n",
    "nltk_tkzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm #para poner el modelo inglés\n",
    "#!python -m spacy download es_core_news_sm # para poner el modelo español\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\") # spacy Inglés "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey\n",
      "@elon.musk\n",
      " \n",
      "this\n",
      "would\n",
      "be\n",
      "so\n",
      "cool\n",
      "in\n",
      "synergy\n",
      "with\n",
      "what\n",
      "we\n",
      "do\n",
      "\n",
      "\n",
      "@ExoWandercraft\n",
      ",\n",
      "already\n",
      "letting\n",
      "the\n",
      "walking\n",
      "impaired\n",
      "walk\n",
      "\n",
      "\n",
      "autonomously\n",
      "http://tinyurl.com/3zaj9xqs\n",
      ".\n",
      "Hit\n",
      "us\n",
      "up\n",
      "!\n",
      ";)\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(tweet)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gramas\n",
    "\n",
    "En nuestro contexto un $n$-grama es una secuencia de $n$ tokens extraidos de un documentos. Estas secuencias nos permiten incluir en el vocabulario términos que están relacionados y aparecen juntos de forma recurrente. Por ejemplo la siguiente frase:\n",
    "\n",
    "*Nueva York es la ciudad más poblada de los Estados Unidos*\n",
    "\n",
    "Los $n$-gramas son importantes por que nos ayudan a conservar el significaco, por ejemplo los términios **Nueva York** y **Estados Unidos** adquieren diferente significado a si son separados. Si extendemos el vocabulario mediante la inclusión del $n$-gramas nuestro sistema de procesamiento de lenguaje natural podra retener parte del contexto (order y significado) en el texto.\n",
    "\n",
    "Como resultado del uso de $n$-gramas,  ahora se debe determinar cuales aportan de ellos la mayor cantidad de información, y así poder reducir la cantidad de $n$-gramas) incluidos en el vocabulario. Esto nos ayudará a reconocer *\"Nueva York\"*, sin considerar terminos  *\"nueva sociedad\"*. Más adelante revisaremos a detalle estrategías para identificar $n$-gramas relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema con utilizar $n$-gramas que habrá muchas tokens irrelevantes lo que hará que nuestro vocabulario cresca desmedidamente.  Por ejemplo de la frase anterior el $2$-grama  *\"york es\"* es muy porbable que no proporcione infomación relevante para nuestro sistema de PLN.  Si los  $n$-gramas son extremadamente raros, no tienen ninguna correlación con otras palabras que puedan usar para ayudar a identificar temas que conecten documentos o clases de documentos. Por lo general la mayoría de los $2$-gramas son bastante raros, más aún los de 3 y 4 tokens.\n",
    "\n",
    "Si se utilizan $n$-gramas de forma indiscriminada la dimensionalidad del vector de características podría facilmente sobrepasar el tamaño de los documentos, lo cual sería contraproducente. Más adelane en el curso revisaremos algunas técnicas estadisticas para determinar $n$-gramas relevantes. \n",
    "\n",
    "También los $n$-gramas muy comunes pueden generar un incoveniente para los sistemas de PNL. Considere el bigrama \"de los\" de la frase anterior. Ese tipo de bigramas, seguramente apaarecen en la mayoría de sus documentos. El hecho de que se a común hace que pierda su utilidad para discriminar entre documentos, lo cual resulta en poco poder predictivo. Al igual cualquier otro token, los $n$-gramas generalmente debería omitirse si ocurren con demasiada frecuencia. Por ejemplo, si un token o $n$-grama aparece en más del 25% de los documentos del corpus, podría no incluirse como parte del vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words (palabras vacías)\n",
    "Este término se refiere a palabras comunes, es decir que ocurren con una frecuencia alta y que por lo general contienen poca información del significado de una frase. Ejemplos de algunas palabras des este tipo son:  artículos (el, las, los, un), preposiciones (sin, por, para ...) y conectores (y, o, entonces, etc).  \n",
    "\n",
    "Tradicionalmente, es común que los sistemas de PLN excluyan las stop words del vocabulario para reducir su complejidad. Sin embargo, a pesar de que las stop words aportan poca información, estás pueden aportar información relevante cuando forman parte de $n$-gramas.\n",
    "\n",
    "Por ejemplo en las siguientes oraciones:\n",
    "\n",
    "- se requiere tener celular y computadora \n",
    "- se requiere tener celular o computadora\n",
    "\n",
    "En el ejemplo previo si se utilizan tri-gramas y se remueven las stop words se tendría el token *tener celular computadora*, mientras que si no se remuven y se generan $4$-gramas se generarían los elementos  *tener celular y computadora* y *tener celular o computadora*, con lo cual se decribe mejor el siginificado de cada clase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remover o no las stop words depende de cada aplicación particular. Aún cuando el no remover las stop words podría reducir el vocabulario, la realidad es que estas no representan más en unos pocos cientos de palabras (al rededor de 300 para español). El omitirlas tendrá poco impacto cuando se utilizan solo $1$-gramas y aún mucho menor cuando $n>1$. Por ejemplo si asumimos que un corpus de utilizan 20,000 palabras el quitar las stop words nos dejará con alrededor de 19700 terminos en nuestro vocabulario. Cuando se utilizan bigramas el vocabulario resultante incluiria millones de terminos con lo que el ahorro de memoria sería aún mucho menor.  \n",
    "Debido a lo anterior, si tiene suficientes recursos de memoría y procesamiento es más recomendable no remover las stop words. \n",
    "\n",
    "También dependiendo de que tanta información quiera conservar o descartar es posible solo elegir un subconjunto de stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=20000-500\n",
    "n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial as fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199990000.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fac(N)/(fac(n)*(fac(N-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190115250.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fac(N)/(fac(n)*(fac(N-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lista de stop words nltk\n",
    "import nltk\n",
    "#nltk.download('stopwords') #Solo ejecutar la primera vez\n",
    "stop_words = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)# numero de stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_sin_stop_words=[x for x in tokens_sin_puntuacion if x not in  stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_sin_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_sin_puntuacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista de stop words spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in tokens_sin_puntuacion if x not in  nlp.Defaults.stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización\n",
    "\n",
    "Como ya hemos mencionado, el tamaño del vocabulario impacta en el desempeño de un sistema de PLN. Otra estrategia ampliamente utilizada para la reducción del vocabulario es la normalización de términos. Por ejemplo la sustitución del los nombres de usuario en el un tweet. Esto ayuda a conseguir que lo tokens que significan cosas similares se combinen en una única forma *normalizada*. Lo cual reduce el tamaño del vocabulario y también mejora la asociación de significados entre esas diferentes formas de un token o $n$-grama. El tener un vocabulario reducido ayuda a reducir el overfitting de los sistemas de aprendizaje. Por ahora la unica normalización que realizaremos será **case folding**\n",
    "\n",
    "### Case folding \n",
    "\n",
    "Esta técnica consiste uniformizar las palabra que solo difieren en el uso de mayúsculas. Utilizamos mayúsculas es al inicio de un parráfo o después de un punto, en nombres propios o para dar énfasis, por lo que al hacer case folding podríamos estar perdiendo información. Sin  embargo, esta normalización ayuda a reducir el vocabulario y ayuda a unificar las palabras que pretenden significar lo mismo (y que se escriban igual) en una unidad del vocabulario. \n",
    "\n",
    "Case folding puede provocar que dos palabras con diferentes significados sean representados por el mismo token. Por ejemplo en las frases:\n",
    "\n",
    "- *El Papa León XIII falleció a los 93 años*\n",
    "- *El león es el rey de la selva*\n",
    "\n",
    "En ambos casos la palabra *león* es un sustantivo, pero en el primer caso es un nombre propio.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Normalización\n",
    "\n",
    "Como mecionamos en secciones anteriores, una forma de mantener \"*limitado*\" el tamaño del vocabulario es mediante la normalización del texto. Ya revisasmo algunas normalizacione simples como `case folding`, eliminación `stop words` o remplazando números o nombres de usuario. Una forma más sofisticada es mediante la unificación de palabras que tienen significados similares, es decir reemplazar todos términos con significado muy similar con único token. Una foma de conseguir este último tip de normalización es mediante la identificación de patrones (`stem/lemma`) comumes entre diferentes formas de una palabra/token. Por ejemplo las palabras `hidraulico` e `hidrico` (note que en los ejemplos se omiten los acentos, lo cual también podría ser un tipo de normalización) compartene el stem `hidr`; y las palabras `personal` y `personas` comparten el lemma `persona`.   Stemming y Lematización son dos formas de conseguir la unificación de palabras similares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "El proceso de Stemming puede entendese como un proceso de eliminación de los sufijos, esto con el de combinar palabras con significados similares bajo un stem común. A dfierencia de la lematización no se requiere que el stem sea raíz valida de la palabra que este escrita correctamente. Solamente es un sino  símbolo o token, que representa varias posibles formas de una \"misma\" palabra.\n",
    "\n",
    "Tanto Stemming como Lematización permiten en cierta medida, que los algoritmos de aprendizaje puedan sumarizar la diferentes formas ortografícas y gramaticales por ejemplo distintos tiempos \n",
    "de un verbo (`jugar`, `jugando`, `jugamos`...) o singulares y plurales (`persona`, `personas`), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo simple de stemer sería uno que sutituyera las formas plurales. Por facilida consideremos que solo se remueve la letra `s` al final de las palabras. Lo podríamos implementar como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_plural=re.compile(r'^(.*ss|.*?)(s)?$')\n",
    "def naive_stemmer(tokens):\n",
    "   return [remove_plural.findall(token)[0][0] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amigo', 'de', 'fox']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_stemmer(['amigos','de','fox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lo', 'amigo', 'de', 'mi', 'amigo']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_stemmer(['los','amigos','de','mis','amigos']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro stemer simple solo considera una regla del español, la cual es que los plurales terminan en `s`. Aunque implicitamente estaría decidiendo en base a dos reglas:\n",
    "\n",
    "1. Si la palabra termina en `s` el singular es la palabras con la letra `s` removida. \n",
    "2. En caso contrario la palabra ya está en singular.\n",
    "\n",
    "Para implementar un stemer robusto es necesario considerar muchas reglas o bien aprenderlo de un corpus etiquetado utilizando Entity reconigtion. Afortunadamente las librerías como **NLTK** y **sPaCy** ya cuentan con implementaciónes de steamers y lematizadores para differentes idiomas. \n",
    "\n",
    "Dos de los algoritmos de stemming más son los propuestos por el matemático Martin Porter. El primero el PorterStemmer, y el denomidado SnowballStemmer que es una mejora del PorterStemmer.\n",
    "\n",
    "Mientras PorterStemmer, es un algoritmo enfocado en elimar las términaciones morfológicas e inflexiones comunes de las palabras en inglés (puede utilizarse en español), el SnowballStemmer añade reglas dependientes del idioma que se utiliza y es el que utilizaremos para español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jhon', 'wash', 'the', 'dish']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "[stemmer.stem(w) for w in ['jhon','washes', 'the','dishes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fli', 'fli', 'randomli']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(w) for w in ['flies','fly', 'randomly']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yo', 'estaría', 'temporalment', 'ocupado']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# efecto parcial\n",
    "[stemmer.stem(w) for w in ['yo','estaría', 'temporalmente','ocupado']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probemos el SnowBallStemmer para español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snstemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yo', 'prodr', 'temporal', 'ocup']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aplica más reglas del español\n",
    "[snstemmer.stem(w) for w in ['yo','prodría', 'temporalmente','ocupado']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estudiante curioso puede echar un ojo a una implementación en Python del stemmer de Porter  en el siguiente repositorio:\n",
    "\n",
    "https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización\n",
    "\n",
    "A diferencia del stemming, la Lematización reduce las palabras a un `lemma` es decir un stem que tiene la forma  de una palabra que válida del lenguaje, es decir que es semántaticamnete correcto . Por ejemplo las conjugaciones `Programando`, `Programaré` serian reducidos al lemma `programar`. Por lo que en un lematizador reduce a palabras que aparecen en el diccionario, por tanto las reglas del mismos siguen las reglas gramaticales de forma más estrita que en un stemmer. \n",
    "\n",
    "Mientras **NLTK** si proporciona implementaciones para lematizadores y stemmers, **spacy** solo cuenta con lematizaodres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('wordnet') # Solo  la primera vez que se utiliza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fly', 'fly', 'randomly']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nltk_lemmatizer.lemmatize(w) for w in ['flies','fly', 'randomly']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_english = spacy.load(\"en_core_web_sm\") # spacy Inglés\n",
    "nlp_spanish = spacy.load(\"es_core_news_sm\") # spacy Español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fly', 'fly', 'randomly']\n"
     ]
    }
   ],
   "source": [
    "doc= nlp_english('flies fly randomly')\n",
    "lemmas=[token.lemma_.lower() for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yo', 'poder', 'temporalmente', 'ocupar']\n"
     ]
    }
   ],
   "source": [
    "doc1= nlp_spanish('yo podría temporalmente ocupado')\n",
    "lemmas=[token.lemma_.lower() for token in doc1]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto stemming como lematización reducen el tamaño de su vocabulario, y al mismo tiempo mantienen información del significado. Lo anterior ayuda a reduccir la dimensión de las representaciones vectotiales, así como a generalizar su modelo de lenguaje. Los lematizadores/stemmers pueden utilizarse sin problema en aplicaciones donde no se requiera distinguir entre palabras que comparten la misma raíz, su uso resulta en reducciones significativas del tamaño del vocabulario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuando normalizar\n",
    "\n",
    "Algunas consideraciones a tener en cuenta para cuando utilizar stemmers/lematizadores son: Los Stemmers son generalmente son más rápidos, más simples de implementar. Los stemmers introducen  más errores y derivarán un número mucho mayor de palabras, esto implica una pérdida de información. Una consecuencia de la reducción del vovabulario es el incremento en la ambigüedad, este efecto se presenta tanto lematizadores como los stemmers. Sin embargo, los lematizadores retenienen mejor la información relativa al significado y uso de las palabras dentro del texto. \n",
    "\n",
    "El uso de la normalización en sistemas de recuperación de información  resulta en un mayor recall y en una redución de la precisión y el accuracy del sistema. Esto, ya que como consecuencia de la compresión del vocabulario se obtienen  muchos documentos que no son relevantes para los significados originales de las palabras. Dado que los resultados de la búsqueda se pueden clasificar en función de la relevancia, los motores de búsqueda frecuentemenete utilizan stemming y lematización  para aumentar la probabilidad de incluir resultados relevantes. Sin embargo, combinan los resultados de la búsqueda con versiones donde no realiza una normalización para clasificar los resultados de búsqueda que le presentan.\n",
    "\n",
    "Por otro lado, para un chatbot donde la precisión es importante. Un chatbot primero debería elegir las versiones no normalizadas y recurrir las versiones normalizadas solo si no hay coincidencias exactas.\n",
    "\n",
    "Se comienda el uso lematizadores/stemmers en problemas donde se cuente con una cantidad limitada y que no contenga  palabras donde la capitalización es importante que, así como en los que el lenguaje es limitado (por ejemplo un subcampo muy pequeño de la ciencia, la tecnología o la literatura) o donde se utilza mucho lenguaje informal (twitter, facebook, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Tarea</b>:<br />\n",
    "    \n",
    "Implemtar una clase con los siguientes métodos:\n",
    " <ul>\n",
    "     <li>Método fit(corpus) </li>\n",
    "<li>tokenizer(corpus, ngrmas=[],case_folding=True,user_replace=True,derivacion='stemming',...))que incluya las siguientes caraterísticas:</li>\n",
    "    <ul>\n",
    "    <li>Dividir los tokens por signos de puntuación y espacios</li>\n",
    "    <li>Especificar mediante una lista los diferentes tamaños de n-gramas (por ejemplo si ngram=[2,4]  generará tokens de tamaño 1, 2 y 4). Simpre genera los 1-grama.</li>\n",
    "      <li>Normalizaciones (variables boleanas True/False)</li>\n",
    "        <ul>\n",
    "            <li>Case folding </li>\n",
    "            <li>Remplazo de nombre de usuario @user-&gt;&lt;user&gt;)</li>\n",
    "            <li>Remplazo de nombre de números 3424-&gt;&lt;number&gt;</li>\n",
    "            <li>Remplazo de nombre hiper-vinculos http://XXX-&gt;&lt;link&gt;</li>\n",
    "            <li>Incluir soporte para lematización y stemming</li>\n",
    "        </ul>\n",
    "    </ul><br />\n",
    "     <li>Una función transform(corpus) que reciba un corpus y regrese un lista con la representación de bolsa de palabras para cada documento</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\"el rey de constantinopla esta constantinoplizado.\",\n",
    "        \"consta que constanza no lo pudo desconstantinoplizar.\",\n",
    "        \"el desconstantinoplizador que desconstantinoplizare al rey de constantinopla\",\n",
    "        \"buen desconstantinoplizador será\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario={}\n",
    "for i,doc in enumerate(corpus):\n",
    "    #aplicar normalizacion\n",
    "    for word in doc.split():\n",
    "        vocabulario[word]=vocabulario.get(word,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc=list(vocabulario.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['al',\n",
       " 'buen',\n",
       " 'consta',\n",
       " 'constantinopla',\n",
       " 'constantinoplizado.',\n",
       " 'constanza',\n",
       " 'de',\n",
       " 'desconstantinoplizador',\n",
       " 'desconstantinoplizar.',\n",
       " 'desconstantinoplizare',\n",
       " 'el',\n",
       " 'esta',\n",
       " 'lo',\n",
       " 'no',\n",
       " 'pudo',\n",
       " 'que',\n",
       " 'rey',\n",
       " 'será']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminos=[t for t,c in voc]\n",
    "terminos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokenizados=[]\n",
    "for i,doc in enumerate(corpus):\n",
    "    #aplicar normalizacion\n",
    "    tokens={}\n",
    "    for word in doc.split():\n",
    "        tokens[word]=1\n",
    "    doc_tokenizados.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>el</th>\n",
       "      <th>rey</th>\n",
       "      <th>de</th>\n",
       "      <th>constantinopla</th>\n",
       "      <th>esta</th>\n",
       "      <th>constantinoplizado.</th>\n",
       "      <th>consta</th>\n",
       "      <th>que</th>\n",
       "      <th>constanza</th>\n",
       "      <th>no</th>\n",
       "      <th>lo</th>\n",
       "      <th>pudo</th>\n",
       "      <th>desconstantinoplizar.</th>\n",
       "      <th>desconstantinoplizador</th>\n",
       "      <th>desconstantinoplizare</th>\n",
       "      <th>al</th>\n",
       "      <th>buen</th>\n",
       "      <th>será</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   el  rey  de  constantinopla  esta  constantinoplizado.  consta  que  \\\n",
       "0   1    1   1               1     1                    1       0    0   \n",
       "1   0    0   0               0     0                    0       1    1   \n",
       "2   1    1   1               1     0                    0       0    1   \n",
       "3   0    0   0               0     0                    0       0    0   \n",
       "\n",
       "   constanza  no  lo  pudo  desconstantinoplizar.  desconstantinoplizador  \\\n",
       "0          0   0   0     0                      0                       0   \n",
       "1          1   1   1     1                      1                       0   \n",
       "2          0   0   0     0                      0                       1   \n",
       "3          0   0   0     0                      0                       1   \n",
       "\n",
       "   desconstantinoplizare  al  buen  será  \n",
       "0                      0   0     0     0  \n",
       "1                      0   0     0     0  \n",
       "2                      1   1     0     0  \n",
       "3                      0   0     1     1  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(doc_tokenizados).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

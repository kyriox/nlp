{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulario\n",
    "\n",
    "El primer paso dentro del procesmaiento de lenguage natural implica la construcción de un vocabulario. El objetivo es convertir secuencias de texto en unidades *minimas* de escritura con *significado* (tokens). Para este curso usaremos palabras, $n$-gramas y $q$-gramas. Sin embargo, las técnicas de preprocesamiento pueden ser extendidas facilmente a equaciones, emoticones o cualquier otra unidad de escritura.   \n",
    "\n",
    "Como lidiamos con lenguaje escrito, la obtención de los tokens requiere manipulación de cadenas de caracteres. Se requiere identificar puntuación, signo diacríticos (dieresis, acententos), en el caso del Inglés es posible que se desee dividir las contracciones (You're -> you are). Una vez construido el vocabulario mediante la **tokenización** de todos los documentos, puede  realizarse una reducción del mismo mediante un proceso de **lematización** o **steamming**. Ya con el vocabulario es posible construir una representación vectorial. \n",
    "\n",
    "Note que la tokenización podría ser a nível de sufijos/prefijos, silabas o incluso letras, pero por el momento solo lidiaremos con palabras. También es posible construir unidades formadas por 2,3 o $n$ palabras, a esto tokens se les conoce como $n$-gramas y nos permitene incluir conceptos que de más de una unidad por ejemplo en inglés *ice cream*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizador\n",
    "\n",
    "Es proceso de tokenizado es un proceso de segmentación de *documentos*. Donde la segmentación es dividir el texto (información no estructurada) en unidades más pequeñas que pueden ser contabilizadas de forma discreta. El resultado de la contabilización de las ocurrencias de cada término puede ser utilizada directamente como un representación vectorial del documento. Con lo cual se transforma una entrada de información no estructurada en información estructurada que puedes ser utilizada por algoritmos de aprendizaje automático. La aplicación más común de este tipo de vectores (**bag of words**) para recuperación de documentos o búsqueda. \n",
    "\n",
    "El tokenizador más simple consiste en utilizar el espacio en blanco como delimitador para definir los tokens en terminos de palabras. En python se puede hacer como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pepe',\n",
       " 'pecas',\n",
       " 'pica',\n",
       " 'papas',\n",
       " 'con',\n",
       " 'un',\n",
       " 'pico,',\n",
       " 'con',\n",
       " 'un',\n",
       " 'pico',\n",
       " 'pepe',\n",
       " 'pecas',\n",
       " 'pica',\n",
       " 'papas.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"Pepe pecas pica papas con un pico, con un pico pepe pecas pica papas.\"\n",
    "doc.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el  método *split* pareciera se tiene un tokenizador medianamente bueno, sin embargo hay al menos dos situaciones no deseadas la primera los tokes *pico,*  y *papas.* esos tokens incluyen signos de puntuación, además los tokens como *Pepe* y *pepe* serán considerados como elementos distintos. Un tokenizador mas soifisticado debería separar los tokens de la puntuación, por ahora dejaremos este cuestión pero la retomaremos más adelante. Una forma básica de obtener una representación númerica de una secuencia de texto es mediante una representación binaria de cada token que existe en el vocabulario, está representación es conocida como **one-hot vectors**. Cada sentencia es representada como una lista de one-hot vectors por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del vocabulario: 7\n"
     ]
    }
   ],
   "source": [
    "## obtenemos el vocabulario a partir\n",
    "doc1=\"Pepe pecas pica papas con un pico\"\n",
    "vocabulario=str.split(doc1) #Utilizamos el tokenizador\n",
    "## se ordena\n",
    "vocabulario.sort()\n",
    "n=len(vocabulario)\n",
    "print(f\"El tamaño del vocabulario: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pepe', 'con', 'papas', 'pecas', 'pica', 'pico', 'un']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada one-hot vector binaria será del tamaño del vocabulario y tendrá solo un 1 en la posición que corresponde a la palabra que representa.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "## Creamos la tabla de one-hot vectors \n",
    "one_hot_vectors=np.zeros((n,n)) \n",
    "## generamos la representación vectorial para nuesta frase de ejemplo\n",
    "for i,w in enumerate(doc1.split()):\n",
    "    one_hot_vectors[i,vocabulario.index(w)]=1\n",
    "one_hot_vectors=one_hot_vectors.astype(int)\n",
    "print(one_hot_vectors) # ya tenemos un vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pepe</th>\n",
       "      <th>con</th>\n",
       "      <th>papas</th>\n",
       "      <th>pecas</th>\n",
       "      <th>pica</th>\n",
       "      <th>pico</th>\n",
       "      <th>un</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pepe</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pecas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pica</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>papas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>un</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pico</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pepe con papas pecas pica pico un\n",
       "Pepe     1                             \n",
       "pecas                    1             \n",
       "pica                          1        \n",
       "papas              1                   \n",
       "con          1                         \n",
       "un                                    1\n",
       "pico                               1   "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Podemos ver la misma información en un dataFrame para hacerlo un poco mas legible\n",
    "df = pd.DataFrame(one_hot_vectors, columns=vocabulario, index=doc1.split())\n",
    "df[df == 0] = '' #remplazamos los 0 con la cadena vacia\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para nuestro ejemplo tenemos una matriz de 7x7 ya que el vocabulario está solo constituido por una única sentencia. Recuerde que un 1 indica que el token si es parte del documento y un 0 que ese temino no se encuentra en el mismo. Este tipo de estructura es eficiente para determinar si un palabra es o no parte del un documento (solo debe verse si la fila está activa en la columna correspondiente). Además siempre es posible reconstruir el documento original, con lo cual no se pierde información. Este tipo de representación es frecuentemente utilizado en redes neuronales y modelado de lenguajes. \n",
    "\n",
    "La importancia de este tipo de representación es que se ha transformado una sentencia escrita en lenguaje natural, a un espacio donde es posible que una *máquina* realice operaciones matemáticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las desventajas de tener una representación matricial de los one-hot vectors es que debido a que son altamenete disperso, su almacenamiento puede ser ineficianete si se hace de forma matricial; mientras que si lo hacemos mediante listas o alguna otra estructura dispersa se tiene un incremento en la complejida de las operaciones. \n",
    "\n",
    "En el caso del idioma castellano se considera que existen al rededor de unas 100,000 palabras. Teniendo lo anterior en cuenta, un one-hot vector para una palabra dada tendría que ser un vector de dimensión 10000 con único elemento diferente de 0. \n",
    "<div class=\"alert alert-success\">\n",
    "    <ul>\n",
    "<li>¿Que costo tendría almacenar nuestra frase de ejemplo sin utilizar una representación dispersa que considere todas las palabras del idioma castellano? </li>\n",
    "<li>¿Un documento con 100 tokens?</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000000000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*20000*4*1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considere que se utilizan 4 bytes por caracter o entero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "Usar un representación one-hot requiere grandes cantidades de memoria, otro posible enfoque es sumarizar la información en la tabla en un solo vector. Lo anterior reduciria la memoria requerida pero se perdería la información del orden en que aparecen en el documento (de ahi el nombre de bag of words). Aún con la desventaja anterior este vector conservaria los conceptos que aparecen en el documento, sería semejante a un índice de terminos en un libro.\n",
    "\n",
    "La sumarización puede realizarse mediante un vector binario que solo indique si la palabra aparece o no en el documento, o bien contando el número de veces que aparece cada término (vector de frecuencias)\n",
    "\n",
    "Considere el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pepe</th>\n",
       "      <th>con</th>\n",
       "      <th>papas</th>\n",
       "      <th>pecas</th>\n",
       "      <th>pica</th>\n",
       "      <th>pico</th>\n",
       "      <th>un</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pecas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pica</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>papas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>un</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pico</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>un</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pico</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pecas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pica</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>papas</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pepe con papas pecas pica pico un\n",
       "pecas                    1             \n",
       "pica                          1        \n",
       "papas              1                   \n",
       "con          1                         \n",
       "un                                    1\n",
       "pico                               1   \n",
       "con          1                         \n",
       "un                                    1\n",
       "pico                               1   \n",
       "pecas                    1             \n",
       "pica                          1        \n",
       "papas              1                   "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2=\"pecas pica papas con un pico con un pico pecas pica papas\"\n",
    "## Creamos la tabla de one-hot vectors \n",
    "one_hot_vectors2=np.zeros((len(doc2.split()),n)) \n",
    "## generamos la representación vectorial para nuesta frase de ejemplo\n",
    "for i,w in enumerate(doc2.split()):\n",
    "    one_hot_vectors2[i,vocabulario.index(w)]=1\n",
    "df2 = pd.DataFrame(one_hot_vectors2, columns=vocabulario, index=doc2.split())\n",
    "df2[df2 == 0] = '' #remplazamos los 0 con la cadena vacia\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el one-hot encoding podemos generar el vector binario como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(one_hot_vectors2, axis=0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "también con el la tabla one-hot podemos generar el vector de frecuencias como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(one_hot_vectors2, axis=0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La memoria requerida  por representación previa aún depende aún depende del tamaño del vocabulario y sigue siendo prohibitiva para grandes cantidades de datos. Una forma más ecónomica en memoria es utilizar una lista asociativa(diccionario) como  sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pepe': 1, 'pecas': 1, 'pica': 1, 'papas': 1, 'con': 1, 'un': 1, 'pico': 1}\n",
      "{'pecas': 1, 'pica': 1, 'papas': 1, 'con': 1, 'un': 1, 'pico': 1}\n"
     ]
    }
   ],
   "source": [
    "#version binaria\n",
    "doc1_bow = {token: 1 for token in doc1.split()}\n",
    "print(doc1_bow)\n",
    "doc2_bow = {token: 1 for token in doc2.split()}\n",
    "print(doc2_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario=['pepe','con','papas','pecas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[[(0,1),(3,1)],[(5,1),(10,4)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pecas': 2, 'pica': 2, 'papas': 2, 'con': 2, 'un': 2, 'pico': 2}\n"
     ]
    }
   ],
   "source": [
    "#version frecuencia\n",
    "doc2_bowf={}\n",
    "for token in doc2.split():\n",
    "   doc2_bowf[token]=doc2_bowf.get(token,0)+1\n",
    "print(doc2_bowf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pepe</th>\n",
       "      <th>pecas</th>\n",
       "      <th>pica</th>\n",
       "      <th>papas</th>\n",
       "      <th>con</th>\n",
       "      <th>un</th>\n",
       "      <th>pico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pepe  pecas  pica  papas  con  un  pico\n",
       "doc1     1      1     1      1    1   1     1\n",
       "doc2     0      1     1      1    1   1     1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow=pd.DataFrame([doc1_bow,doc2_bow], index=['doc1','doc2']).fillna(0).astype(int)\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almacenar a la información de esta última forma es mucho más eficiente en memoria ya que cada documento ya que solo se consideran las palabras persentes en el documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora practiquemos con un ejemplo que conste de una colección de documentos (corpus), considere el siguiente conjunto de sentencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\"el rey de constantinopla esta constantinoplizado.\",\n",
    "        \"consta que constanza no lo pudo desconstantinoplizar.\"\n",
    "        \"el desconstantinoplizador que desconstantinoplizare al rey de constantinopla\",\n",
    "        \"buen desconstantinoplizador será\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto punto\n",
    "\n",
    "El producto punto de dos vectores o producto escalar se calcula multiplicando todos los elementos de un vector por todos los elementos del segundo vector y luego sumando cada uno de los resultados del producto. \n",
    "\n",
    "En python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([5, 4, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.dot(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(v1*v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x1 * x2 for x1, x2 in zip(v1, v2)]) # ineficiente pero ilustrativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "¿Cómo podemos medir que tan similares son dos documentos?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La representación binaria de bag of words es un espacio vectorial (VSM) obtenido a partir de documentos en lenguaje natural (oraciones). En este espacio es posible realizar productos escalares, así como otras operaciones vectoriales como: suma, resta, *and*, *or*, medias, etc. También nos permite medir similitud/distancia entre documentos (i.e. distancia euclidiana,el ángulo entre vectores, etc). Como sabemos los procesadores utilizan expresiones binarias que son utilizadas para realizar indizado y realizar búsquedas de forma eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejorando el tokenizador\n",
    "\n",
    "Es frecuente, que se desee utilizar como separadores de tokens utilizan caracteres diferentes  a en una oración. Además de que nuestro rokenizador mantiene los signos de puntuación en las palabras. Una posible solución sería dividir el texto no solo en espacios en blanco, sino también en puntuación (comas, puntos, comillas, signos de amiración, etc). Sin embargo, en  algunos casos, podría desearse tratar lo signos como tokens independientes o tal vez simplemente se quiera ignorarlos.\n",
    "\n",
    "Una de incluir diferentes patrones de división es meidante el uso de expresiones regulares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expresiones regulares\n",
    "\n",
    "Recordemos que las expresiones sirver para expresar lenguajes regulares, y en Python se pueden utilizar mediante la librería **re**. Rvisaremos brevemente alguno aspectos del su uso:\n",
    "\n",
    "- Los corchetes ([x]) se utilizan para indicar una tipo o un conjunto de caracteres. \n",
    "- El signo + después del corchete de cierre (]) indica que debe haber al menos una coincidencia de los caracteres dentro de los corchetes. \n",
    "- El signo * indica que cero o mas coincidencias de los caracteres dentro de la clase. \n",
    "- El símbolo  \\s dentro es una clase predefinida que incluye todos los espacios en blanco como  [espacio], [tabulador]. Los seis caracteres de espacio en blanco son espacio (' '), tabulación ('\\ t'), return ('\\ r'), nueva línea ('\\n') y  ('\\f').\n",
    "- Para indicar un rango de caracteres se utiliza el signo menos (-). Por ejemplo \\[1-9\\] indica la clase  [123456789], [a-zA-Z] hace match con los rangos de minusculas y mayusculas de los caracteres alfanuméricos. \n",
    "- Los parentesis son utilizado para agrupar expresiones regulares.\n",
    "- Para expresar que se desea hacer match con -, debe ponerse justo después del corchete abierto para la clase de carácter. En caso contrario el analizador lo tomara como un rango de caracteres. \n",
    "- Los caracters especiales se pueden escapar utilizando una barra invertida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Python la biblioteca *re* permite compilar las expresiones regulares, con lo que se obtiene un tokenizador más eficiente. \n",
    "\n",
    "Las expresiones regulares también nos permiten realizar normalizaciones de texto complejas, por ejemplo extrar hyper-vínculos, direcciones de correo, nombres de usuario etc. Veremos un ejemplo sencillo de normalización y retomaremos el tema más adelante en el curso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "## Una expresión regular que divide utilizando signos de puntuación y espacios en blanco\n",
    "patron_tokenizer=re.compile(r\"([-\\s.,;¿?¡!])+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilizaremos la siguiente frase tomada del poema Día trece de Ramón Lopez Velarde\n",
    "poema=\"\"\"¿En qué embriaguez bogaban tus pupilas para que así pudiesen narcotizarlo todo? \n",
    "          Tu tiniebla guiaba mis latidos, cual guiabala columna de fuego al israelita.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=patron_tokenizer.split(poema)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '¿', 'En', ' ', 'qué', ' ', 'embriaguez', ' ', 'bogaban', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:10]) #Los últimos 13 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver tenemos tenemos espacios en blanco, por lo que requerimos filtar los caracteres que no sean de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tu', 'tiniebla', 'guiaba', 'mis', 'latidos', 'cual', 'guiabala', 'columna', 'de', 'fuego', 'al', 'israelita']\n"
     ]
    }
   ],
   "source": [
    "no_deseados=['-',' ','\\t','\\n','.',';',',','¿','?','¡','!','']\n",
    "tokens_sin_puntuacion=[x for x in tokens if x not in no_deseados]\n",
    "print(tokens_sin_puntuacion[-12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hace match con todos las cadena que comienzan con @ y contienen al menos un caracter más\n",
    "patron_user=re.compile(r\"(@[a-zA-Z0-9\\.]+)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet=\"\"\"Hey @elon.musk  this would be so cool in synergy with what we do \n",
    "@ExoWandercraft, already letting the walking impaired walk \n",
    "autonomously http://tinyurl.com/3zaj9xqs. Hit us up! ;)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey<user>  this would be so cool in synergy with what we do \n",
      "<user>, already letting the walking impaired walk \n",
      "autonomously http://tinyurl.com/3zaj9xqs. Hit us up!\n"
     ]
    }
   ],
   "source": [
    "tweetu=patron_user.sub('<user>',tweet)\n",
    "print(tweetu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EJERCICIO</b>:\n",
    "Definir una expresión regular que remplace los hipervínculos con la cadena <br /> \n",
    "&lt;link &gt;\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tokenizador puede ser tan complejo como se desee y puede querer adaptarse a una tarea especifíca. Por ejemplo en un tweet podríamos tratar de forma especial los caracteres XD. \n",
    "Existen muchas librerías en python que implementan tokenizadores especializados (dominio, idioma, etc). Las dos que mostraremos en este curso son **spaCy** y **NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " '@elon',\n",
       " '.',\n",
       " 'musk',\n",
       " 'this',\n",
       " 'would',\n",
       " 'be',\n",
       " 'so',\n",
       " 'cool',\n",
       " 'in',\n",
       " 'synergy',\n",
       " 'with',\n",
       " 'what',\n",
       " 'we',\n",
       " 'do',\n",
       " '@ExoWandercraft',\n",
       " ',',\n",
       " 'already',\n",
       " 'letting',\n",
       " 'the',\n",
       " 'walking',\n",
       " 'impaired',\n",
       " 'walk',\n",
       " 'autonomously',\n",
       " 'http://tinyurl.com/3zaj9xqs',\n",
       " '.',\n",
       " 'Hit',\n",
       " 'us',\n",
       " 'up',\n",
       " '!',\n",
       " ';)']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLTK tokenizador de tweers \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk_tkzr=TweetTokenizer()\n",
    "nltk_tkzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm #para poner el modelo inglés\n",
    "#!python -m spacy download es_core_news_sm # para poner el modelo español\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\") # spacy Inglés "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey\n",
      "@elon.musk\n",
      " \n",
      "this\n",
      "would\n",
      "be\n",
      "so\n",
      "cool\n",
      "in\n",
      "synergy\n",
      "with\n",
      "what\n",
      "we\n",
      "do\n",
      "\n",
      "\n",
      "@ExoWandercraft\n",
      ",\n",
      "already\n",
      "letting\n",
      "the\n",
      "walking\n",
      "impaired\n",
      "walk\n",
      "\n",
      "\n",
      "autonomously\n",
      "http://tinyurl.com/3zaj9xqs\n",
      ".\n",
      "Hit\n",
      "us\n",
      "up\n",
      "!\n",
      ";)\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(tweet)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gramas\n",
    "\n",
    "En nuestro contexto un $n$-grama es una secuencia de $n$ tokens extraidos de un documentos. Estas secuencias nos permiten incluir en el vocabulario términos que están relacionados y aparecen juntos de forma recurrente. Por ejemplo la siguiente frase:\n",
    "\n",
    "*Nueva York es la ciudad más poblada de los Estados Unidos*\n",
    "\n",
    "Los $n$-gramas son importantes por que nos ayudan a conservar el significaco, por ejemplo los términios **Nueva York** y **Estados Unidos** adquieren diferente significado a si son separados. Si extendemos el vocabulario mediante la inclusión del $n$-gramas nuestro sistema de procesamiento de lenguaje natural podra retener parte del contexto (order y significado) en el texto.\n",
    "\n",
    "Como resultado del uso de $n$-gramas,  ahora se debe determinar cuales aportan de ellos la mayor cantidad de información, y así poder reducir la cantidad de $n$-gramas) incluidos en el vocabulario. Esto nos ayudará a reconocer *\"Nueva York\"*, sin considerar terminos  *\"nueva sociedad\"*. Más adelante revisaremos a detalle estrategías para identificar $n$-gramas relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema con utilizar $n$-gramas que habrá muchas tokens irrelevantes lo que hará que nuestro vocabulario cresca desmedidamente.  Por ejemplo de la frase anterior el $2$-grama  *\"york es\"* es muy porbable que no proporcione infomación relevante para nuestro sistema de PLN.  Si los  $n$-gramas son extremadamente raros, no tienen ninguna correlación con otras palabras que puedan usar para ayudar a identificar temas que conecten documentos o clases de documentos. Por lo general la mayoría de los $2$-gramas son bastante raros, más aún los de 3 y 4 tokens.\n",
    "\n",
    "Si se utilizan $n$-gramas de forma indiscriminada la dimensionalidad del vector de características podría facilmente sobrepasar el tamaño de los documentos, lo cual sería contraproducente. Más adelane en el curso revisaremos algunas técnicas estadisticas para determinar $n$-gramas relevantes. \n",
    "\n",
    "También los $n$-gramas muy comunes pueden generar un incoveniente para los sistemas de PNL. Considere el bigrama \"de los\" de la frase anterior. Ese tipo de bigramas, seguramente apaarecen en la mayoría de sus documentos. El hecho de que se a común hace que pierda su utilidad para discriminar entre documentos, lo cual resulta en poco poder predictivo. Al igual cualquier otro token, los $n$-gramas generalmente debería omitirse si ocurren con demasiada frecuencia. Por ejemplo, si un token o $n$-grama aparece en más del 25% de los documentos del corpus, podría no incluirse como parte del vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words (palabras vacías)\n",
    "Este término se refiere a palabras comunes, es decir que ocurren con una frecuencia alta y que por lo general contienen poca información del significado de una frase. Ejemplos de algunas palabras des este tipo son:  artículos (el, las, los, un), preposiciones (sin, por, para ...) y conectores (y, o, entonces, etc).  \n",
    "\n",
    "Tradicionalmente, es común que los sistemas de PLN excluyan las stop words del vocabulario para reducir su complejidad. Sin embargo, a pesar de que las stop words aportan poca información, estás pueden aportar información relevante cuando forman parte de $n$-gramas.\n",
    "\n",
    "Por ejemplo en las siguientes oraciones:\n",
    "\n",
    "- se requiere tener celular y computadora \n",
    "- se requiere tener celular o computadora\n",
    "\n",
    "En el ejemplo previo si se utilizan tri-gramas y se remueven las stop words se tendría el token *tener celular computadora*, mientras que si no se remuven y se generan $4$-gramas se generarían los elementos  *tener celular y computadora* y *tener celular o computadora*, con lo cual se decribe mejor el siginificado de cada clase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remover o no las stop words depende de cada aplicación particular. Aún cuando el no remover las stop words podría reducir el vocabulario, la realidad es que estas no representan más en unos pocos cientos de palabras (al rededor de 300 para español). El omitirlas tendrá poco impacto cuando se utilizan solo $1$-gramas y aún mucho menor cuando $n>1$. Por ejemplo si asumimos que un corpus de utilizan 20,000 palabras el quitar las stop words nos dejará con alrededor de 19700 terminos en nuestro vocabulario. Cuando se utilizan bigramas el vocabulario resultante incluiria millones de terminos con lo que el ahorro de memoria sería aún mucho menor.  \n",
    "Debido a lo anterior, si tiene suficientes recursos de memoría y procesamiento es más recomendable no remover las stop words. \n",
    "\n",
    "También dependiendo de que tanta información quiera conservar o descartar es posible solo elegir un subconjunto de stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=20000-500\n",
    "n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial as fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199990000.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fac(N)/(fac(n)*(fac(N-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190115250.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fac(N)/(fac(n)*(fac(N-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lista de stop words nltk\n",
    "import nltk\n",
    "#nltk.download('stopwords') #Solo ejecutar la primera vez\n",
    "stop_words = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)# numero de stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_sin_stop_words=[x for x in tokens_sin_puntuacion if x not in  stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_sin_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_sin_puntuacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista de stop words spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in tokens_sin_puntuacion if x not in  nlp.Defaults.stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización\n",
    "\n",
    "Como ya hemos mencionado, el tamaño del vocabulario impacta en el desempeño de un sistema de PLN. Otra estrategia ampliamente utilizada para la reducción del vocabulario es la normalización de términos. Por ejemplo la sustitución del los nombres de usuario en el un tweet. Esto ayuda a conseguir que lo tokens que significan cosas similares se combinen en una única forma *normalizada*. Lo cual reduce el tamaño del vocabulario y también mejora la asociación de significados entre esas diferentes formas de un token o $n$-grama. El tener un vocabulario reducido ayuda a reducir el overfitting de los sistemas de aprendizaje. Por ahora la unica normalización que realizaremos será **case folding**\n",
    "\n",
    "### Case folding \n",
    "\n",
    "Esta técnica consiste uniformizar las palabra que solo difieren en el uso de mayúsculas. Utilizamos mayúsculas es al inicio de un parráfo o después de un punto, en nombres propios o para dar énfasis, por lo que al hacer case folding podríamos estar perdiendo información. Sin  embargo, esta normalización ayuda a reducir el vocabulario y ayuda a unificar las palabras que pretenden significar lo mismo (y que se escriban igual) en una unidad del vocabulario. \n",
    "\n",
    "Case folding puede provocar que dos palabras con diferentes significados sean representados por el mismo token. Por ejemplo en las frases:\n",
    "\n",
    "- *El Papa León XIII falleció a los 93 años*\n",
    "- *El león es el rey de la selva*\n",
    "\n",
    "En ambos casos la palabra *león* es un sustantivo, pero en el primer caso es un nombre propio nombre propio.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EJERCICIOS</b>:<br />\n",
    "    \n",
    "Implemtar una clase con los siguientes métodos:\n",
    " <ul>\n",
    "     <li>Método fit(corpus) </li>\n",
    "<li>tokenizer(corpus, ngrmas=[],case_folding=True,user_replace=True,...))que incluya las siguientes caraterísticas:</li>\n",
    "    <ul>\n",
    "    <li>Dividir los tokens por signos de puntuación y espacios</li>\n",
    "    <li>Especificar mediante una lista los diferentes tamaños de n-gramas (por ejemplo si ngram=[2,4]  generará tokens de tamaño 1, 2 y 4). Simpre genera los 1-grama.</li>\n",
    "      <li>Normalizaciones (variables boleanas True/False)</li>\n",
    "        <ul>\n",
    "            <li>Case folding </li>\n",
    "            <li>Remplazo de nombre de usuario @user-&gt;&lt;user&gt;)</li>\n",
    "            <li>Remplazo de nombre de números 3424-&gt;&lt;number&gt;</li>\n",
    "            <li>Remplazo de nombre hiper-vinculos http://XXX-&gt;&lt;link&gt;</li>\n",
    "        </ul>\n",
    "    </ul><br />\n",
    "     <li>Una función transform(corpus) que reciba un corpus y regrese un lista con la representación de bolsa de palabras para cada documento</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizador:\n",
    "    def __init__(self,corpus=[],ngrams=[2],case_folding=True):\n",
    "        self.vocabulario={\"token\": count}\n",
    "        if corpus:\n",
    "            self.vocabulario=_construye_vocabulario(corpus)\n",
    "            \n",
    "    def _construye_vocabulario(corpus):\n",
    "        ## Aplicar normalizaciones al corpus\n",
    "        ## construir tabla de vocabulario\n",
    "        pass\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "           self.vocabulario=_construye_vocabulario(corpus) \n",
    "            \n",
    "    def tokenizador(self, corpus):\n",
    "        ## Aplicar normalizaciones al texto\n",
    "        ## Tokenizar el texto utilizando el vocabulario \n",
    "        #(decidir que hacer cuando ocurren palabras que no se encuentres en el vocabulario)\n",
    "        ## regresar lista de tokens\n",
    "        pass\n",
    "    \n",
    "    def transform(self, corpus):\n",
    "        ## tokenizar corpus\n",
    "        ## generar vectores \n",
    "        ## regresar dataframe (solo ilustrativo, nos quedaremos con la representación en forma de dict)\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\"el rey de constantinopla esta constantinoplizado.\",\n",
    "        \"consta que constanza no lo pudo desconstantinoplizar.\",\n",
    "        \"el desconstantinoplizador que desconstantinoplizare al rey de constantinopla\",\n",
    "        \"buen desconstantinoplizador será\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario={}\n",
    "for i,doc in enumerate(corpus):\n",
    "    #aplicar normalizacion\n",
    "    for word in doc.split():\n",
    "        vocabulario[word]=vocabulario.get(word,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'el': 2,\n",
       " 'rey': 2,\n",
       " 'de': 2,\n",
       " 'constantinopla': 2,\n",
       " 'esta': 1,\n",
       " 'constantinoplizado.': 1,\n",
       " 'consta': 1,\n",
       " 'que': 2,\n",
       " 'constanza': 1,\n",
       " 'no': 1,\n",
       " 'lo': 1,\n",
       " 'pudo': 1,\n",
       " 'desconstantinoplizar.': 1,\n",
       " 'desconstantinoplizador': 2,\n",
       " 'desconstantinoplizare': 1,\n",
       " 'al': 1,\n",
       " 'buen': 1,\n",
       " 'será': 1}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokenizados=[]\n",
    "for i,doc in enumerate(corpus):\n",
    "    #aplicar normalizacion\n",
    "    tokens={}\n",
    "    for word in doc.split():\n",
    "        tokens[word]=1\n",
    "    doc_tokenizados.append(tokens)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'el': 1,\n",
       "  'rey': 1,\n",
       "  'de': 1,\n",
       "  'constantinopla': 1,\n",
       "  'esta': 1,\n",
       "  'constantinoplizado.': 1},\n",
       " {'consta': 1,\n",
       "  'que': 1,\n",
       "  'constanza': 1,\n",
       "  'no': 1,\n",
       "  'lo': 1,\n",
       "  'pudo': 1,\n",
       "  'desconstantinoplizar.': 1},\n",
       " {'el': 1,\n",
       "  'desconstantinoplizador': 1,\n",
       "  'que': 1,\n",
       "  'desconstantinoplizare': 1,\n",
       "  'al': 1,\n",
       "  'rey': 1,\n",
       "  'de': 1,\n",
       "  'constantinopla': 1},\n",
       " {'buen': 1, 'desconstantinoplizador': 1, 'será': 1}]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFramedoc_tokenizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
